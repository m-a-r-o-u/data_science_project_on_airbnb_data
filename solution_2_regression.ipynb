{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "\n",
    "Content\n",
    "\n",
    "- Linear Regression with One Feature\n",
    "\n",
    "    - ...\n",
    "\n",
    "- Linear Regression with Multiple Features\n",
    "\n",
    "    - ...\n",
    "\n",
    "ToDo:\n",
    "\n",
    "- ...\n",
    "\n",
    "Additional Material:\n",
    "\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Linear Regression\n",
    "- linear regression\n",
    "- one variable\n",
    "- residuals\n",
    "- R2\n",
    "- f-test, p-value\n",
    "\n",
    "Feature Selection\n",
    "- forwarnd/backward feature selection\n",
    "\n",
    "Data Transormation\n",
    "- outliers\n",
    "- normality\n",
    "- log transforamtion\n",
    "- binning + one hot encoding\n",
    "\n",
    "Missing Values\n",
    "- binning imputation?\n",
    "- missing indicator\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Central Concepts\n",
    "\n",
    "All our inputs need to be **numeric** for linear regression.\n",
    "\n",
    "**Linear Regression Assumptions**\n",
    "\n",
    "   1. **Linearity**:\\\n",
    "   A linear correlation between the input and the target variable is assumed.\\\n",
    "   Otherwise, change the model.\n",
    "   \n",
    "   2. **Normality**:\\\n",
    "   Normal distributed input variables.\\\n",
    "   Otherwise, apply variable transformations.\n",
    "   \n",
    "   3. **No Multicorrealinity**\\\n",
    "   No correlation between input variables.\n",
    "   Otherwise, the model can't identify the correct coeficients.\n",
    "\n",
    "   4. **Homoscedasticity**:\\\n",
    "   Constant variance for the sample distribution.\n",
    "   Otherwise, the model's robustness suffers.\n",
    "\n",
    "**RMSE Metric**\n",
    "\n",
    "$ rmse = \\sqrt{\\frac{1}{n}\\sum (y_i - \\hat{y}_i)^{2}}$\n",
    "\n",
    "**R2 Metric**\n",
    "\n",
    "$r^{2} = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^{2}}{\\sum (y_i - \\bar{y}_i)^{2}}$\n",
    "\n",
    "- r2 < 0.3 --- none or very weak effect size\n",
    "- 0.3 < r2 < 0.5 --- weak or low effect size\n",
    "- 0.5 < r2 < 0.7 --- moderate effect size\n",
    "- 0.7 < r2 < 1.0 --- strong effect size\n",
    "\n",
    "**Z Score**\n",
    "\n",
    "**Skew**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dark_plot_theme = True\n",
    "\n",
    "if dark_plot_theme:\n",
    "    plt.style.use('dark_background')\n",
    "\n",
    "\n",
    "# pandas display settings\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import d2_tweedie_score\n",
    "\n",
    "#Scikit-learn is designed for prediction, while statsmodels is more suited for explanatory analysis.\n",
    "import statsmodels.api as sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload ./utils.py\n",
    "\n",
    "import importlib\n",
    "import utils\n",
    "importlib.reload(utils)\n",
    "from utils import get_dichotomous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Feature Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assume: no nulls, no outliers (z>3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the directory and load the data\n",
    "\n",
    "cwd = Path()\n",
    "\n",
    "ipath = cwd / 'data'\n",
    "\n",
    "ipath.mkdir(exist_ok=True)\n",
    "\n",
    "ifile = ipath / 'features.csv'\n",
    "\n",
    "data = pd.read_csv(ifile, index_col=['id'])\n",
    "\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check for null values\n",
    "\n",
    "data.isnull().values.any() # REMOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with One Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out a simple linear regression: predict the price from square meters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variable names\n",
    "\n",
    "target = 'price'\n",
    "features = ['square_meter']\n",
    "\n",
    "variables = [target] + features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot target distribution: price and log-price\n",
    "\n",
    "figsize = (12, 5)\n",
    "fig, axs = plt.subplots(1, 2, figsize=figsize)\n",
    "\n",
    "# price distribution\n",
    "d = data[target]\n",
    "skew = d.skew()\n",
    "title = f'skewness: {skew:0.2f}'\n",
    "sns.histplot(d, bins=50, ax=axs[0]).set(title=title);\n",
    "\n",
    "# log-price distribution\n",
    "d = np.log(data[target]+1)\n",
    "skew = d.skew()\n",
    "title = f'skewness: {skew:0.2f}'\n",
    "sns.histplot(d, bins=50, ax=axs[1]).set(title=title);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature distribution: sqm and log-sqm\n",
    "\n",
    "figsize = (12, 5)\n",
    "fig, axs = plt.subplots(1, 3, figsize=figsize)\n",
    "\n",
    "d = data[features[0]]\n",
    "mask = data[f'imp_{features[0]}']+data[f'imp_z_{features[0]}'] < 1\n",
    "\n",
    "# sqm distribution\n",
    "skew = d.skew()\n",
    "title = f'skewness: {skew:0.2f}'\n",
    "sns.histplot(d, bins=50, ax=axs[0]).set(title=title);\n",
    "\n",
    "# sqm distribution without imputation\n",
    "d = d[mask]\n",
    "skew = d.skew()\n",
    "title = f'skewness: {skew:0.2f}'\n",
    "sns.histplot(d, bins=50, ax=axs[1]).set(title=title);\n",
    "\n",
    "# log sqm distribution\n",
    "d = np.log(d+1)\n",
    "skew = d.skew()\n",
    "title = f'skewness: {skew:0.2f}'\n",
    "sns.histplot(d, bins=50, ax=axs[2]).set(title=title);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define regression metrics\n",
    "\n",
    "def rmse(*args, **kwargs):\n",
    "    return mean_squared_error(*args, **kwargs, squared=False)\n",
    "\n",
    "rmetrics = {}\n",
    "rmetrics['r2_score'] = r2_score\n",
    "rmetrics['mean_squared_error'] = mean_squared_error\n",
    "rmetrics['rmse'] = rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop null values\n",
    "\n",
    "rdata = data[variables]\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(\n",
    "    rdata.drop(target, axis=1), rdata[target], random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply LinearRegression and predict\n",
    "\n",
    "# apply\n",
    "lr = LinearRegression()\n",
    "lr.fit(xtrain, ytrain)\n",
    "\n",
    "# predict\n",
    "ypred = lr.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicit metric calculation calculation\n",
    "\n",
    "r2 = 1 - np.sum(np.square(ytest - ypred)) / np.sum(np.square(ytest - np.mean(ytest)))\n",
    "rmse = np.sqrt(np.mean(np.square(ytest - ypred)))\n",
    "\n",
    "print(f'R-squared: {r2:.2f}')\n",
    "print(f'RMSE:      {rmse:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_wrapper(xtrain, xtest, ytrain, ytest, data, show=True):\n",
    "    '''\n",
    "    Convenience function wrapping:\n",
    "    application of linear regression\n",
    "    presentation of metrics and\n",
    "    plotting the results\n",
    "    '''\n",
    "\n",
    "    # fit the linear regression model\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(xtrain, ytrain)\n",
    "\n",
    "    # apply the model\n",
    "    ypred = lr.predict(xtest)\n",
    "\n",
    "    # print and plot results\n",
    "    if show:\n",
    "        print('*** model paramters:')\n",
    "        print('coeff.: ', ', '.join([f'{x:.3f}' for x in lr.coef_]))\n",
    "        print(f'inter.: {lr.intercept_:.3f}')\n",
    "        print()\n",
    "        print('*** scores:')\n",
    "        for k, v in rmetrics.items():\n",
    "            score = v(ytest, ypred)\n",
    "            print(f'{k:22} {score:.3f}')\n",
    "\n",
    "        plot = sns.jointplot(data=data, x='square_meter', y='price', marker='.', marginal_kws=dict(bins=25));\n",
    "        plot.ax_joint.plot(xtest, ypred, '-', color='violet' );\n",
    "    return ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and plot\n",
    "\n",
    "ypred = regression_wrapper(xtrain, xtest, ytrain, ytest, rdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the above analysis but drop imputed values\n",
    "\n",
    "REWRITE:\\\n",
    "The integer columns **z_score_price** and **z_score_square_meter**\\\n",
    "indicate outliers with a zscore > 3 with 1\\\n",
    "(otherwise 0).\n",
    "\n",
    "\n",
    "Tips:\\\n",
    "Sum up the zscores: z_score_price and z_score_square_meter\\\n",
    "Create a boolean mask for zscores that sum up larger than 1\\\n",
    "Apply the mask and drop nans: using where and dropna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the imputed variables\n",
    "\n",
    "pattern = '^imp.*({}|{})$'.format(*variables)\n",
    "mask = data.filter(regex=pattern, axis=1).sum(axis=1) < 1\n",
    "\n",
    "rdata = rdata.where(mask).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(\n",
    "    rdata.drop(target, axis=1), rdata[target], random_state=0)\n",
    "\n",
    "# fit and plot\n",
    "\n",
    "ypred = regression_wrapper(xtrain, xtest, ytrain, ytest, rdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log transform the variables using: np.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log transformation\n",
    "\n",
    "rdata = np.log(rdata+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(\n",
    "    rdata.drop(target, axis=1), rdata[target], random_state=0)\n",
    "\n",
    "# fit model / plot results again\n",
    "ypred = regression_wrapper(xtrain, xtest, ytrain, ytest, rdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unlog\n",
    "\n",
    "ypred_exp = np.exp(ypred) - 1\n",
    "ytest_exp = np.exp(ytest) - 1\n",
    "\n",
    "r2 = 1 - np.sum(np.square( ypred_exp - ytest_exp )) / np.sum(np.square( ytest_exp.mean() - ytest_exp ))\n",
    "rmse = np.sqrt(np.mean(np.square(ypred_exp - ytest_exp)))\n",
    "\n",
    "print(f'r2   = {r2:.2f}')\n",
    "print(f'rmse = {rmse:.2f}')\n",
    "\n",
    "fig, ax = plt.subplots();\n",
    "sns.scatterplot(data=np.exp(rdata)-1, x='square_meter', y='price', ax=ax);\n",
    "ax.scatter(np.exp(xtest)-1, np.exp(ypred)-1, color='violet');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the log data\n",
    "\n",
    "rdata = (rdata - rdata.mean()) / rdata.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(\n",
    "    rdata.drop(target, axis=1), rdata[target], random_state=0)\n",
    "\n",
    "# fit model / plot results again\n",
    "ypred = regression_wrapper(xtrain, xtest, ytrain, ytest, rdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residuals with:\n",
    "# LOESS (locally estimated scatterplot smoothing)\n",
    "\n",
    "tmp = rdata[variables]\n",
    "tmp['residuals'] = (ytest - ypred)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.residplot(data=tmp, x='square_meter', y='residuals', lowess=True, line_kws=dict(color='red'), ax=ax);\n",
    "ax.axis('equal');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the R2\n",
    "\n",
    "- r2 < 0.3 is considered a None or Very weak effect size,\n",
    "\n",
    "- 0.3 < r2 < 0.5 is considered a weak or low effect size,\n",
    "\n",
    "- 0.5 < r2 < 0.7 is considered a Moderate effect size,\n",
    "\n",
    "- 0.7 < r2 < 1.0 is considered strong effect size,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Multiple Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation\n",
    "\n",
    "cor = np.abs(data.drop(get_dichotomous(data), axis=1).corr())\n",
    "\n",
    "# absolute correlation\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "sns.heatmap(cor, annot=False, cmap=plt.cm.Blues, vmin=0, vmax=1, ax=axs[0]);\n",
    "\n",
    "# absolution correlation > 0.7\n",
    "sns.heatmap(cor.where(cor>0.7, other=0), annot=False, cmap=plt.cm.Blues, vmin=0, vmax=1, ax=axs[1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only numberc data\n",
    "\n",
    "target = 'price'\n",
    "\n",
    "rdata = data.select_dtypes(include=[np.number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward feature selection\n",
    "\n",
    "x = rdata.drop(target, axis=1)\n",
    "y = rdata[target]\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, random_state=0)\n",
    "\n",
    "# forward feature selection\n",
    "print('*** selected features:')\n",
    "max_features = 16\n",
    "features = []\n",
    "for i in range(1, max_features):\n",
    "    estimator = LinearRegression()\n",
    "    #estimator = RandomForestRegressor()\n",
    "    selector = SelectFromModel(estimator, max_features=i).fit(xtrain, ytrain)\n",
    "\n",
    "    # Only keep the best columns\n",
    "    mask = selector.get_support()\n",
    "    cnames = xtrain.columns[mask]\n",
    "    features.append(cnames)\n",
    "\n",
    "    print(i, ', '.join(list(cnames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression for all the feature sets\n",
    "\n",
    "r2s = []\n",
    "for feature in features:\n",
    "    variables = list(feature) + [target]\n",
    "    tmp = rdata[variables]\n",
    "\n",
    "    x = tmp.drop(target, axis=1)\n",
    "    y = tmp[target]\n",
    "\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(x, y, random_state=0)\n",
    "\n",
    "    lr = LinearRegression()\n",
    "    #lr = Lasso()\n",
    "    #lr = RandomForestRegressor()\n",
    "    lr.fit(xtrain, ytrain)\n",
    "\n",
    "    ypred = lr.predict(xtest)\n",
    "    r2 = r2_score(ytest, ypred)\n",
    "\n",
    "    # calculate r2 adjusted\n",
    "    n = np.shape(ytest)[0]\n",
    "    k = len(feature) + 1\n",
    "    r2_adj = 1 - (1 - r2) * (n - 1) / (n - k)\n",
    "    \n",
    "    print(', '.join(variables))\n",
    "    print(f'r2={r2:.3f} f2_adj={r2_adj:.3f}')\n",
    "    print()\n",
    "    r2s.append(r2_adj)\n",
    "\n",
    "\n",
    "x = rdata.drop(target, axis=1)\n",
    "y = rdata[target]\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, random_state=0)\n",
    "\n",
    "lr = LinearRegression()\n",
    "#lr = Lasso()\n",
    "#lr = RandomForestRegressor()\n",
    "lr.fit(xtrain, ytrain)\n",
    "\n",
    "ypred = lr.predict(xtest)\n",
    "r2 = r2_score(ytest, ypred)\n",
    "\n",
    "# calculate r2 adjusted\n",
    "n = np.shape(ytest)[0]\n",
    "k = len(rdata.columns)\n",
    "r2_adj = 1 - (1 - r2) * (n - 1) / (n - k)\n",
    "\n",
    "print(', '.join(variables))\n",
    "print(f'r2={r2:.3f} f2_adj={r2_adj:.3f}')\n",
    "print()\n",
    "r2s.append(r2_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "ax.plot(r2s)\n",
    "ax.set_title('$r^2_{adj}$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log transformation on data skew\n",
    "\n",
    "# exclude categorical data\n",
    "tmp = data.select_dtypes(include=[np.number])\n",
    "tmp = tmp.drop(get_dichotomous(tmp), axis=1)\n",
    "\n",
    "skew = pd.DataFrame(tmp.skew(), columns=['skew'])\n",
    "skew['log_skew'] = np.log(tmp + 1).skew()\n",
    "skew['log_skew/skew'] = np.abs(skew['log_skew'] / skew['skew'] * 100)\n",
    "\n",
    "display(skew.sort_values('log_skew/skew'))\n",
    "\n",
    "# columns: 40% less skew from log\n",
    "mask = skew['log_skew/skew'] < 60\n",
    "log_columns = skew[mask].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply log transform\n",
    "\n",
    "# drop old variables\n",
    "rdata = rdata.drop(log_columns, axis=1)\n",
    "\n",
    "# create new log variables\n",
    "new_log_columns = [f'log_{v}' for v in log_columns]\n",
    "rdata[new_log_columns] = np.log(data[log_columns] + 1)\n",
    "\n",
    "target = 'log_price'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward feature selection\n",
    "\n",
    "x = rdata.drop(target, axis=1)\n",
    "y = rdata[target]\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, random_state=0)\n",
    "\n",
    "# forward feature selection\n",
    "print('*** selected features:')\n",
    "max_features = 16\n",
    "features = []\n",
    "for i in range(1, max_features):\n",
    "    estimator = LinearRegression()\n",
    "    #estimator = RandomForestRegressor()\n",
    "    selector = SelectFromModel(estimator, max_features=i).fit(xtrain, ytrain)\n",
    "\n",
    "    # Only keep the best columns\n",
    "    mask = selector.get_support()\n",
    "    cnames = xtrain.columns[mask]\n",
    "    features.append(cnames)\n",
    "\n",
    "    print(i, ', '.join(list(cnames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression for all the feature sets\n",
    "\n",
    "r2s_log = []\n",
    "for feature in features:\n",
    "    variables = list(feature) + [target]\n",
    "    tmp = rdata[variables]\n",
    "\n",
    "    x = tmp.drop(target, axis=1)\n",
    "    y = tmp[target]\n",
    "\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(x, y, random_state=0)\n",
    "\n",
    "    lr = LinearRegression()\n",
    "    #lr = Lasso()\n",
    "    #lr = RandomForestRegressor()\n",
    "    lr.fit(xtrain, ytrain)\n",
    "\n",
    "    ypred = lr.predict(xtest)\n",
    "    r2 = r2_score(np.exp(ytest)-1, np.exp(ypred)-1)\n",
    "\n",
    "    # calculate r2 adjusted\n",
    "    n = np.shape(ytest)[0]\n",
    "    k = len(feature) + 1\n",
    "    r2_adj = 1 - (1 - r2) * (n - 1) / (n - k)\n",
    "    \n",
    "    print(', '.join(variables))\n",
    "    print(f'r2={r2:.3f} f2_adj={r2_adj:.3f}')\n",
    "    print()\n",
    "    r2s_log.append(r2_adj)\n",
    "\n",
    "\n",
    "x = rdata.drop(target, axis=1)\n",
    "y = rdata[target]\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, random_state=0)\n",
    "\n",
    "lr = LinearRegression()\n",
    "#lr = Lasso()\n",
    "#lr = RandomForestRegressor()\n",
    "lr.fit(xtrain, ytrain)\n",
    "\n",
    "ypred = lr.predict(xtest)\n",
    "r2 = r2_score(np.exp(ytest)-1, np.exp(ypred)-1)\n",
    "\n",
    "# calculate r2 adjusted\n",
    "n = np.shape(ytest)[0]\n",
    "k = len(rdata.columns)\n",
    "r2_adj = 1 - (1 - r2) * (n - 1) / (n - k)\n",
    "\n",
    "print(', '.join(variables))\n",
    "print(f'r2={r2:.3f} f2_adj={r2_adj:.3f}')\n",
    "print()\n",
    "r2s_log.append(r2_adj)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the last data point jumps and contains all features\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4), sharey=True)\n",
    "\n",
    "axs[0].plot(r2s)\n",
    "axs[0].set_title('$r^2_{adj}$');\n",
    "\n",
    "axs[1].plot(r2s_log)\n",
    "axs[1].set_title('$r^2_{adj}$ with log');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize (makes results worse)\n",
    "\n",
    "#norm_columns = rdata.drop(get_dichotomous(tmp), axis=1).columns\n",
    "#rdata[norm_columns] = (rdata[norm_columns] - rdata[norm_columns].mean()) / rdata[norm_columns].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('3o10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "24bd5b2c4284f955ab7628ddca6a5f285d231065025c4cec3682ee9df201cb6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
