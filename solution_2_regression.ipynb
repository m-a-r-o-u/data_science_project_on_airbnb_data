{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Linear Regression\n",
    "---\n",
    "\n",
    "**Content**\n",
    "\n",
    "- Linear Regression with One Feature\n",
    "\n",
    "\n",
    "- Linear Regression with Multiple Features\n",
    "\n",
    "\n",
    "**Additional Material**:\n",
    "\n",
    "- interactive linear regression exploration [here](https://observablehq.com/@yizhe-ang/interactive-visualization-of-linear-regression)\n",
    "\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dark_plot_theme = True\n",
    "\n",
    "if dark_plot_theme:\n",
    "    plt.style.use('dark_background')\n",
    "\n",
    "\n",
    "# pandas display settings\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "# utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import explained_variance_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload ./utils.py\n",
    "\n",
    "import importlib\n",
    "import utils\n",
    "importlib.reload(utils)\n",
    "from utils import get_dichotomous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Feature Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assume: no nulls, no outliers (z>3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the directory and load the data\n",
    "\n",
    "cwd = Path()\n",
    "\n",
    "ipath = cwd / 'data'\n",
    "\n",
    "ipath.mkdir(exist_ok=True)\n",
    "\n",
    "ifile = ipath / 'features.csv'\n",
    "\n",
    "data = pd.read_csv(ifile, index_col=['id'])\n",
    "\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# check \"data\" for null values.\n",
    "# Use the pandas functions \"isnull\" and \"any\"\n",
    "\n",
    "# TIP:\n",
    "# data...\n",
    "\n",
    "data.isnull().values.any() # REMOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with One Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "All our inputs need to be **numeric** for linear regression.\n",
    "\n",
    "**Linear Regression Assumptions**\n",
    "\n",
    "- Linearity: A linear correlation between the input and the target\n",
    "\n",
    "- Normality: Normal distributed input variables.\n",
    "\n",
    "- No Multicollinearity: Linear independence between variables.\n",
    "\n",
    "- No Auto-Correlation: No correlation between input variables.\n",
    "\n",
    "- Homoscedasticity: Constant variance for the sample distribution.\n",
    "\n",
    "**Metric**\n",
    "\n",
    "- RSME $ = \\sqrt{\\frac{1}{n}\\sum (y_i - \\hat{y}_i)^{2}}$\n",
    "\n",
    "- R2 $ = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^{2}}{\\sum (y_i - \\bar{y}_i)^{2}}$\n",
    "\n",
    "**Z Score**\n",
    "\n",
    "- outlier if:  z-score $ = \\frac{x - \\bar{x}}{\\sigma} > 3$\n",
    "\n",
    "**Skew**\n",
    "\n",
    "- skew $ = \\frac{E[(x - \\bar{x})^3]}{\\sigma^3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the \"price\" from the \"square_meter\"\n",
    "\n",
    "target = 'price'\n",
    "features = ['square_meter']\n",
    "\n",
    "variables = [target] + features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot target distribution: price and log-price\n",
    "\n",
    "figsize = (12, 5)\n",
    "fig, axs = plt.subplots(1, 2, figsize=figsize)\n",
    "\n",
    "# price distribution\n",
    "d = data[target]\n",
    "skew = d.skew()\n",
    "title = f'skewness: {skew:0.2f}'\n",
    "sns.histplot(d, bins=50, ax=axs[0]).set(title=title);\n",
    "\n",
    "# log-price distribution\n",
    "d = np.log(data[target]+1)\n",
    "skew = d.skew()\n",
    "title = f'skewness: {skew:0.2f}'\n",
    "sns.histplot(d, bins=50, ax=axs[1]).set(title=title);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature distribution: sqm and log-sqm\n",
    "\n",
    "figsize = (12, 5)\n",
    "fig, axs = plt.subplots(1, 3, figsize=figsize)\n",
    "\n",
    "d = data[features[0]]\n",
    "mask = data[f'imp_{features[0]}']+data[f'imp_z_{features[0]}'] < 1\n",
    "\n",
    "# sqm distribution\n",
    "skew = d.skew()\n",
    "title = f'skewness: {skew:0.2f}'\n",
    "sns.histplot(d, bins=50, ax=axs[0]).set(title=title);\n",
    "\n",
    "# sqm distribution without imputation\n",
    "d = d[mask]\n",
    "skew = d.skew()\n",
    "title = f'skewness: {skew:0.2f}'\n",
    "sns.histplot(d, bins=50, ax=axs[1]).set(title=title);\n",
    "\n",
    "# log sqm distribution\n",
    "d = np.log(d+1)\n",
    "skew = d.skew()\n",
    "title = f'skewness: {skew:0.2f}'\n",
    "sns.histplot(d, bins=50, ax=axs[2]).set(title=title);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train test split\n",
    "\n",
    "rdata = data[variables]\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(\n",
    "    rdata.drop(target, axis=1), rdata[target], random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression\n",
    "\n",
    "# TODO:\n",
    "# Instantiate the default sklearn\n",
    "# \"LinearRegression\" model\n",
    "\n",
    "# TIP:\n",
    "# lr = ...\n",
    "\n",
    "lr = LinearRegression() # REMOVE\n",
    "\n",
    "# TODO:\n",
    "# Fit the created \"LinearRegression\" model\n",
    "# to \"xtrain\" and \"ytrain\"\n",
    "\n",
    "# TIP:\n",
    "# lr...\n",
    "\n",
    "lr.fit(xtrain, ytrain) # REMOVE\n",
    "\n",
    "# TODO:\n",
    "# Use the fitted model \"lr\"\n",
    "# to make predictions\n",
    "# based on \"xtest\"\n",
    "\n",
    "# TIP:\n",
    "# ypred = lr...\n",
    "\n",
    "ypred = lr.predict(xtest) # REMOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculation of metrics\n",
    "\n",
    "# TODO:\n",
    "# Calculate \"r2\" explicitly\n",
    "# Use \"ytest\" and \"ypred\"\n",
    "# and apply \"np.sum\", \"np.square\" and \"np.mean\"\n",
    "\n",
    "# TIP:\n",
    "# r2 = 1 - ...\n",
    "\n",
    "r2 = 1 - np.sum(np.square(ytest - ypred)) / np.sum(np.square(ytest - np.mean(ytest))) # REMOVE\n",
    "\n",
    "# TODO:\n",
    "# Calculate RMSE explicitly\n",
    "# Use \"ytest\" and \"ypred\"\n",
    "# and apply \"np.sum\", \"np.square\" and \"np.mean\"\n",
    "\n",
    "# TIP:\n",
    "# rmse = ...\n",
    "\n",
    "rmse = np.sqrt(np.mean(np.square(ytest - ypred))) # REMOVE\n",
    "\n",
    "print(f'R-squared: {r2:.2f}')\n",
    "print(f'RMSE:      {rmse:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK\n",
    "\n",
    "if np.round(r2, 2) == 0.07:\n",
    "    print('*** passed, well done!')\n",
    "else:\n",
    "    print('*** r2 is not correct, try again')\n",
    "\n",
    "if np.round(rmse, 2) == 47.74:\n",
    "    print('*** passed, well done!')\n",
    "else:\n",
    "    print('*** rmse is not correct, try again')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression metrics\n",
    "\n",
    "def rmse_score(*args, **kwargs):\n",
    "    return mean_squared_error(*args, **kwargs, squared=False)\n",
    "\n",
    "rmetrics = {}\n",
    "rmetrics['r2'] = r2_score\n",
    "rmetrics['mse'] = mean_squared_error\n",
    "rmetrics['rmse'] = rmse_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_wrapper(xtrain, xtest, ytrain, ytest, data, show=True):\n",
    "    '''\n",
    "    Convenience function wrapping\n",
    "    the application of linear regression\n",
    "    calculation of metrics and\n",
    "    plotting the results\n",
    "    '''\n",
    "\n",
    "    # fit the linear regression model\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(xtrain, ytrain)\n",
    "\n",
    "    # model predictions\n",
    "    ypred = lr.predict(xtest)\n",
    "\n",
    "    # print and plot results\n",
    "    if show:\n",
    "        print('*** model paramters:')\n",
    "        print('coeff.: ', ', '.join([f'{x:.3f}' for x in lr.coef_]))\n",
    "        print(f'inter.: {lr.intercept_:.3f}')\n",
    "        print()\n",
    "        print('*** scores:')\n",
    "        for k, v in rmetrics.items():\n",
    "            score = v(ytest, ypred)\n",
    "            print(f'{k:22} {score:.3f}')\n",
    "\n",
    "        plot = sns.jointplot(data=data, x='square_meter', y='price', marker='.', marginal_kws=dict(bins=25));\n",
    "        plot.ax_joint.plot(xtest, ypred, '-', color='violet' );\n",
    "    return ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and plot\n",
    "\n",
    "ypred = regression_wrapper(xtrain, xtest, ytrain, ytest, rdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop imputations\n",
    "\n",
    "# all imputation masks related to \"price\" and \"square_meter\"\n",
    "pattern = '^imp.*({}|{})$'.format(*variables)\n",
    "\n",
    "\n",
    "# TODO:\n",
    "# Use the regex pattern in \"pattern\"\n",
    "# to filter for the respective feature columns.\n",
    "# \"sum\" the result along \"axis=1\"\n",
    "\n",
    "# TIP:\n",
    "# mask = data.filter(...).sum() < 1\n",
    "\n",
    "mask = data.filter(regex=pattern, axis=1).sum(axis=1) < 1 # REMOVE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Evaluate \"rdata\"\n",
    "# at the calculated \"mask\"\n",
    "# and drop all the imputations.\n",
    "# Use \"where\" and \"dropna\"\n",
    "\n",
    "# TIP:\n",
    "# rdata = rdata.where(...)...\n",
    "\n",
    "rdata = rdata.where(mask).dropna() # REMOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(\n",
    "    rdata.drop(target, axis=1), rdata[target], random_state=0)\n",
    "\n",
    "# fit and plot in the wrapper\n",
    "\n",
    "ypred = regression_wrapper(xtrain, xtest, ytrain, ytest, rdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log transformation\n",
    "\n",
    "# TODO:\n",
    "# Log transform \"rdata\"\n",
    "# Use \"np.log\" and\n",
    "# apply +1 to remove zeros\n",
    "\n",
    "# TIP:\n",
    "# rdata = np.log(...)\n",
    "\n",
    "rdata = np.log(rdata + 1) # REMOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(\n",
    "    rdata.drop(target, axis=1), rdata[target], random_state=0)\n",
    "\n",
    "# fit model / plot results again\n",
    "ypred = regression_wrapper(xtrain, xtest, ytrain, ytest, rdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Revert the log transformation\n",
    "# for \"ytest\" and \"ypred\"\n",
    "# Don't forget -1\n",
    "\n",
    "# TIP:\n",
    "# ytest_exp = ...\n",
    "# ypred_exp = ...\n",
    "\n",
    "ypred_exp = np.exp(ypred) - 1 # REMOVE\n",
    "ytest_exp = np.exp(ytest) - 1 # REMOVE\n",
    "\n",
    "# TODO:\n",
    "# Calculate \"r2\" and \"rmse\"\n",
    "# From \"ytext_exp\" and \"ypred_exp\"\n",
    "# using the functions:\n",
    "# \"r2_score\" and \"rmse_score\"\n",
    "\n",
    "# TIP:\n",
    "# r2 = r2_score(...)\n",
    "# rmse = rmse_score(...)\n",
    "\n",
    "r2 = r2_score(ytest_exp, ypred_exp) # REMOVE\n",
    "rmse = rmse_score(ytest_exp, ypred_exp) # REMOVE\n",
    "\n",
    "print(f'r2   = {r2:.2f}')\n",
    "print(f'rmse = {rmse:.2f}')\n",
    "\n",
    "fig, ax = plt.subplots();\n",
    "sns.scatterplot(data=np.exp(rdata)-1, x='square_meter', y='price', ax=ax);\n",
    "ax.scatter(np.exp(xtest)-1, np.exp(ypred)-1, color='violet');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the log data\n",
    "\n",
    "# TODO:\n",
    "# Standardize \"rdata\"\n",
    "# use the pandas utilities\n",
    "# for \"mean\" and \"std\"\n",
    "\n",
    "# TIP:\n",
    "# rdata = ...\n",
    "\n",
    "rdata = (rdata - rdata.mean()) / rdata.std() # REMOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(\n",
    "    rdata.drop(target, axis=1), rdata[target], random_state=0)\n",
    "\n",
    "# fit model / plot results again\n",
    "ypred = regression_wrapper(xtrain, xtest, ytrain, ytest, rdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residuals with:\n",
    "# LOESS (locally estimated scatterplot smoothing)\n",
    "\n",
    "tmp = rdata[variables]\n",
    "tmp['residuals'] = (ytest - ypred)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# TODO:\n",
    "# Use the seaborn \"residplot\" function\n",
    "# to plot the residuals in stored in \"tmp\"\n",
    "# include, \"lowess=True\"\n",
    "\n",
    "# TIP:\n",
    "# sns.residplot(..., lowess=True, ax=ax)\n",
    "\n",
    "sns.residplot(data=tmp, x='square_meter', y='residuals', lowess=True, line_kws=dict(color='red'), ax=ax); # REMOVE\n",
    "\n",
    "ax.axis('equal');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Linear Regression with Multiple Features\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation\n",
    "\n",
    "corr_data = data.drop(get_dichotomous(data), axis=1)\n",
    "\n",
    "# TODO\n",
    "# Calculate the absolute values\n",
    "# of default correlations matrix.\n",
    "# Use \"np.abs\" and the pandas function \"corr\"\n",
    "# on \"corr_data\"\n",
    "\n",
    "# TIP:\n",
    "# cor = ...\n",
    "\n",
    "cor = np.abs(corr_data.corr()) # REMOVE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot correlations\n",
    "\n",
    "# absolute correlation\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "sns.heatmap(cor, annot=False, cmap=plt.cm.Blues, vmin=0, vmax=1, ax=axs[0]);\n",
    "\n",
    "# TODO:\n",
    "# Plot the absolution correlation\n",
    "# only if corr > 0.7\n",
    "# use \"sns.heatmap\"\n",
    "# but filter for values larger then 0.7\n",
    "# using pandas \"where\" function\n",
    "\n",
    "# TIP:\n",
    "# sns.heatmap(cor.where(...), annot=False, cmap=plt.cm.Blues, vmin=0, vmax=1, ax=axs[1]);)\n",
    "\n",
    "sns.heatmap(cor.where(cor>0.7, other=0), annot=False, cmap=plt.cm.Blues, vmin=0, vmax=1, ax=axs[1]); # REMOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Select for numeric features only\n",
    "# use \"select_dtypes\" on \"data\"\n",
    "# by including only \"np.number\" variables\n",
    "\n",
    "# TIP:\n",
    "# rdata = data...\n",
    "\n",
    "rdata = data.select_dtypes(include=[np.number]) # REMOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "\n",
    "x = rdata.drop(target, axis=1, errors='ignore')\n",
    "y = rdata[target]\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward feature selection\n",
    "\n",
    "print('*** selected features:')\n",
    "max_features = 16\n",
    "features = []\n",
    "for i in range(1, max_features):\n",
    "    # TODO:\n",
    "    # Instantiate \"SelectFromModel\" from sklearn\n",
    "    # with an Lasso() instance as estimator\n",
    "    # and allow for max_features of i.\n",
    "\n",
    "    # TIP:\n",
    "    # selector = SelectFromModel(...)\n",
    "\n",
    "    selector = SelectFromModel(Lasso(), max_features=i) # REMOVE\n",
    "    \n",
    "    selector.fit(xtrain, ytrain)\n",
    "\n",
    "    # Only keep the best columns\n",
    "    mask = selector.get_support()\n",
    "    cnames = xtrain.columns[mask]\n",
    "    features.append(cnames)\n",
    "\n",
    "    print(i, ', '.join(list(cnames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression convenience function\n",
    "\n",
    "def linear_regression_wrapper(data):\n",
    "    x = data.drop(target, axis=1)\n",
    "    y = data[target]\n",
    "\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(x, y, random_state=0)\n",
    "\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(xtrain, ytrain)\n",
    "\n",
    "    ypred = lr.predict(xtest)\n",
    "    r2 = r2_score(ytest, ypred)\n",
    "\n",
    "    # calculate r2 adjusted\n",
    "    n = np.shape(ytest)[0]\n",
    "    k = len(feature) + 1\n",
    "    r2_adj = 1 - (1 - r2) * (n - 1) / (n - k)\n",
    "    \n",
    "    print(', '.join(variables))\n",
    "    print(f'r2={r2:.3f} f2_adj={r2_adj:.3f}')\n",
    "    print()\n",
    "\n",
    "    return r2, r2_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression for the feature sets\n",
    "\n",
    "r2s = []\n",
    "for feature in features:\n",
    "    variables = list(feature) + [target]\n",
    "    \n",
    "    r2, r2_adj = linear_regression_wrapper(rdata[variables])\n",
    "    r2s.append(r2_adj)\n",
    "\n",
    "\n",
    "\n",
    "# TODO\n",
    "# Add the last run including all features.\n",
    "# Apply the \"linear_regression_wrapper\"\n",
    "# defined in the notebook to \"rdata\"\n",
    "\n",
    "# TIP:\n",
    "# r2, r2_adj = ...\n",
    "\n",
    "r2, r2_adj = linear_regression_wrapper(rdata) # REMOVE\n",
    "\n",
    "r2s.append(r2_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "ax.plot(r2s)\n",
    "ax.set_title('$r^2_{adj}$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable skew reduction with log transformation\n",
    "\n",
    "# exclude categorical data\n",
    "tmp = data.select_dtypes(include=[np.number])\n",
    "tmp = tmp.drop(get_dichotomous(tmp), axis=1)\n",
    "\n",
    "skew = pd.DataFrame(tmp.skew(), columns=['skew'])\n",
    "skew['log_skew'] = np.log(tmp + 1).skew()\n",
    "skew['log_skew/skew'] = np.abs(skew['log_skew'] / skew['skew'])\n",
    "\n",
    "display(skew.sort_values('log_skew/skew'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('3o10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "24bd5b2c4284f955ab7628ddca6a5f285d231065025c4cec3682ee9df201cb6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
