{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Model Evaluation and Selection\n",
    "---\n",
    "\n",
    "**Content**:\n",
    "\n",
    "- Regression\n",
    "\n",
    "- Classification\n",
    "\n",
    "- Hyperparamter Tuning\n",
    "\n",
    "- ROC Curve\n",
    "\n",
    "- Lift Curve\n",
    "\n",
    "- SMOTE Classification\n",
    "\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load general dependencies\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import requests\n",
    "\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# tuning\n",
    "from hyperopt import STATUS_OK\n",
    "from hyperopt import fmin\n",
    "from hyperopt import tpe\n",
    "from hyperopt import hp\n",
    "from hyperopt import STATUS_OK\n",
    "from hyperopt import Trials\n",
    "\n",
    "# data preparation\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# feature selection\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# regression metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import d2_tweedie_score\n",
    "\n",
    "# clustering metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# regression models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# classification models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and refresh custom functions\n",
    "\n",
    "import importlib\n",
    "import utils\n",
    "importlib.reload(utils)\n",
    "\n",
    "from utils import adaboost_wrapper\n",
    "from utils import roc_wrapper\n",
    "from utils import plot_cumulative_gains\n",
    "from utils import calc_cumulative_gains\n",
    "from utils import plot_lift_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas display settings\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib color settings\n",
    "\n",
    "dark_plot_theme = True\n",
    "\n",
    "if dark_plot_theme:\n",
    "    plt.style.use('dark_background')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the working directory\n",
    "\n",
    "cwd = Path()\n",
    "\n",
    "ipath = cwd / 'data'\n",
    "\n",
    "ipath.mkdir(exist_ok=True)\n",
    "\n",
    "ifile = ipath / 'features.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import raw dataset\n",
    "\n",
    "cindex = ['id']\n",
    "\n",
    "data = pd.read_csv(ifile, index_col=cindex).sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'price'\n",
    "x = data.drop(target, axis=1).select_dtypes(include=np.number)\n",
    "y = data[target]\n",
    "\n",
    "selector = SelectKBest(f_regression, k=3)\n",
    "features_selected = selector.fit_transform(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_dict = {}\n",
    "selection_dict['score'] = [round(s, 3) for s in selector.scores_]\n",
    "selection_dict['name'] = selector.feature_names_in_\n",
    "selection_dict['p-value'] = selector.pvalues_\n",
    "\n",
    "selection = pd.DataFrame(selection_dict)\n",
    "display(selection.sort_values('score', ascending=False).head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "\n",
    "# set the number of used features\n",
    "n = 15\n",
    "vs = selection.sort_values('score', ascending=False).head(n)['name'].values\n",
    "\n",
    "x = data[vs]\n",
    "y = data[target]\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Application\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression algorithms and metrics\n",
    "rs = 42\n",
    "\n",
    "regs = {}\n",
    "regs['LinearRegression'] = LinearRegression()\n",
    "regs['DecisionTreeRegressor'] = DecisionTreeRegressor(random_state=rs)\n",
    "regs['RandomForestRegressor'] = RandomForestRegressor(random_state=rs)\n",
    "regs['AdaBoostRegressor'] = AdaBoostRegressor(random_state=rs)\n",
    "regs['GradientBoostingRegressor'] = GradientBoostingRegressor(random_state=rs)\n",
    "\n",
    "rmetrics = {}\n",
    "rmetrics['r2_score'] = r2_score\n",
    "rmetrics['mean_squared_error'] = mean_squared_error\n",
    "rmetrics['explained_variance_score'] = explained_variance_score\n",
    "rmetrics['d2_tweedie_score'] = d2_tweedie_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression calculation\n",
    "\n",
    "regression_data = {}\n",
    "for cname, reg in regs.items():\n",
    "    reg.fit(xtrain, ytrain)\n",
    "    ypred = reg.predict(xtest)\n",
    "\n",
    "    classifier_data = {}\n",
    "    for mname, metric in rmetrics.items():\n",
    "        classifier_data[mname] = metric(ytest, ypred)\n",
    "\n",
    "    regression_data[cname] = classifier_data\n",
    "\n",
    "regression_results = pd.DataFrame(regression_data)\n",
    "display(regression_results.T.sort_values('r2_score', ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparamter Tuning**\n",
    "\n",
    "---\n",
    "\n",
    "- improve AdaBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# goal: minimize the objective function\n",
    "\n",
    "def objective(params):\n",
    "    model, r2, mse = adaboost_wrapper(params, xtrain, xtest, ytrain, ytest)\n",
    "    return {'loss': -r2, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the paramer space\n",
    "\n",
    "params = {}\n",
    "params['max_depth'] = hp.choice('max_depth', range(2, 10))\n",
    "params['min_samples_leaf'] = hp.choice('min_samples_leaf', range(1, 250))\n",
    "params['learning_rate'] = hp.uniform('learning_rate', 0.01, 10)\n",
    "params['n_estimators'] = hp.choice('n_estimators', range(5, 100))\n",
    "params['problem'] = 'regression'\n",
    "\n",
    "# record trials\n",
    "trials = Trials()\n",
    "\n",
    "# minimize with fmin\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=params,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials,\n",
    "    return_argmin=False,\n",
    "    )\n",
    "\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show best metrics\n",
    "\n",
    "_, r2, mse = adaboost_wrapper(best, xtrain, xtest, ytrain, ytest)\n",
    "print(f'r2={r2:.3f}, mse={mse:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract tested parameters\n",
    "\n",
    "values = [t['misc']['vals'] for t in trials.trials]\n",
    "\n",
    "trial_params = {}\n",
    "\n",
    "for k, v in values[0].items():\n",
    "    trial_params[k] = [v[k][0] for v in values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot tested parameters\n",
    "\n",
    "figsize = (12, 8)\n",
    "fig, axs = plt.subplots(len(trial_params)+1, figsize=figsize)\n",
    "\n",
    "#losses = np.where(np.array(trials.losses())<0, trials.losses(), np.nan)\n",
    "losses = trials.losses()\n",
    "axs[0].plot(losses)\n",
    "axs[0].set_ylabel('loss')\n",
    "\n",
    "for i, (k, v) in enumerate(trial_params.items()):\n",
    "    axs[i+1].plot(v)\n",
    "    axs[i+1].set_ylabel(k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic Regression\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- Ada Boost Tree\n",
    "- Gradient Boosting Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- define the classification problem\n",
    "- airbnb more expensive then mean\n",
    "- short excurse\n",
    "- show the ROC-Curve AUC-Score\n",
    "- validate on test set\n",
    "- use smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_target(target):\n",
    "    return (target > np.percentile(target, 75)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'price'\n",
    "x = data.drop(target, axis=1).select_dtypes(include=np.number)\n",
    "y = get_classification_target(data[target])\n",
    "\n",
    "selector = SelectKBest(f_classif, k=3)\n",
    "features_selected = selector.fit_transform(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_dict = {}\n",
    "selection_dict['score'] = [round(s, 3) for s in selector.scores_]\n",
    "selection_dict['name'] = selector.feature_names_in_\n",
    "selection_dict['p-value'] = selector.pvalues_\n",
    "\n",
    "selection = pd.DataFrame(selection_dict)\n",
    "display(selection.sort_values('score', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "\n",
    "# set the number of used features\n",
    "n = 5\n",
    "vs = selection.sort_values('score', ascending=False).head(n)['name'].values\n",
    "\n",
    "x = data[vs]\n",
    "y = get_classification_target(data[target])\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Application\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification algorithms and metrics\n",
    "rs = 42\n",
    "\n",
    "classifiers = {}\n",
    "classifiers['LogisticRegression'] = LogisticRegression(random_state=rs)\n",
    "classifiers['DecisionTreeClassifier'] = DecisionTreeClassifier(random_state=rs)\n",
    "classifiers['RandomForestClassifier'] = RandomForestClassifier(random_state=rs)\n",
    "classifiers['AdaBoostClassifier'] = AdaBoostClassifier(random_state=rs)\n",
    "classifiers['GradientBoostingClassifier'] = GradientBoostingClassifier(random_state=rs)\n",
    "\n",
    "cmetrics = {}\n",
    "cmetrics['accuracy_score'] = accuracy_score\n",
    "cmetrics['f1_score'] = f1_score\n",
    "cmetrics['precision_score'] = precision_score\n",
    "cmetrics['recall_score'] = recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification\n",
    "\n",
    "classification_data = {}\n",
    "for cname, reg in classifiers.items():\n",
    "    reg.fit(xtrain, ytrain)\n",
    "    ypred = reg.predict(xtest)\n",
    "    \n",
    "    classifier_data = {}\n",
    "    for mname, metric in cmetrics.items():\n",
    "        classifier_data[mname] = metric(ytest, ypred)\n",
    "\n",
    "    classification_data[cname] = classifier_data\n",
    "\n",
    "test_results = pd.DataFrame(classification_data)\n",
    "display(test_results.T.sort_values('f1_score', ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparamter Tuning**\n",
    "\n",
    "---\n",
    "\n",
    "- improve AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# goal: minimize the objective function\n",
    "\n",
    "def objective(params):\n",
    "    model, f1, acc = adaboost_wrapper(params, xtrain, xtest, ytrain, ytest)\n",
    "    return {'loss': -f1, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the paramer space\n",
    "\n",
    "params = {}\n",
    "params['max_depth'] = hp.choice('max_depth', range(2, 10))\n",
    "params['min_samples_leaf'] = hp.choice('min_samples_leaf', range(1, 250))\n",
    "params['learning_rate'] = hp.uniform('learning_rate', 0.01, 10)\n",
    "params['n_estimators'] = hp.choice('n_estimators', range(5, 100))\n",
    "params['problem'] = 'classification'\n",
    "\n",
    "# record trials\n",
    "trials = Trials()\n",
    "\n",
    "# minimize with fmin\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=params,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=100,\n",
    "    trials=trials,\n",
    "    return_argmin=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show best\n",
    "\n",
    "print(best)\n",
    "model, f1, acc = adaboost_wrapper(best, xtrain, xtest, ytrain, ytest)\n",
    "print(f'f1={f1:.3f}, acc={acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract tested parameters\n",
    "\n",
    "values = [t['misc']['vals'] for t in trials.trials]\n",
    "\n",
    "trial_params = {}\n",
    "\n",
    "for k, v in values[0].items():\n",
    "    trial_params[k] = [v[k][0] for v in values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot tested parameters\n",
    "\n",
    "figsize = (12, 8)\n",
    "fig, axs = plt.subplots(len(trial_params)+1, figsize=figsize)\n",
    "\n",
    "#losses = np.where(np.array(trials.losses())<0, trials.losses(), np.nan)\n",
    "losses = trials.losses()\n",
    "axs[0].plot(losses)\n",
    "axs[0].set_ylabel('loss')\n",
    "\n",
    "for i, (k, v) in enumerate(trial_params.items()):\n",
    "    axs[i+1].plot(v)\n",
    "    axs[i+1].set_ylabel(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = model.predict(xtest)\n",
    "yprob = model.predict_proba(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**roc**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the roc\n",
    "_ = roc_wrapper(ytest, ypred, yprob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**lift**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate lift\n",
    "\n",
    "tmp = pd.DataFrame()\n",
    "tmp['actual'] = ytest\n",
    "tmp['pred'] = ypred\n",
    "tmp['prob'] = yprob[:,1]\n",
    "\n",
    "lift = calc_cumulative_gains(tmp, 'actual', 'pred', 'prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_cumulative_gains(lift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lift_chart(lift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with SMOTE\n",
    "\n",
    "---\n",
    "\n",
    "Synthetic Minority Oversampling Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data using SMOTE\n",
    "\n",
    "_, xtest, _, ytest = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Original dataset shape %s' % Counter(y))\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "\n",
    "x_smote, y_smote = sm.fit_resample(x, y)\n",
    "\n",
    "xtrain, _, ytrain, _ = train_test_split(x_smote, y_smote, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "print('Resampled dataset shape %s' % Counter(y_smote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification with smote\n",
    "\n",
    "classification_data = {}\n",
    "for cname, reg in classifiers.items():\n",
    "    reg.fit(xtrain, ytrain)\n",
    "    ypred = reg.predict(xtest)\n",
    "    \n",
    "    classifier_data = {}\n",
    "    for mname, metric in cmetrics.items():\n",
    "        classifier_data[mname] = metric(ytest, ypred)\n",
    "\n",
    "    classification_data[cname] = classifier_data\n",
    "\n",
    "test_results = pd.DataFrame(classification_data)\n",
    "display(test_results.T.sort_values('f1_score', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('3o10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "24bd5b2c4284f955ab7628ddca6a5f285d231065025c4cec3682ee9df201cb6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
